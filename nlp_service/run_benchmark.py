from nlp_engine import PureSkinNLPEngine
import json
import time
import numpy as np
import logging
from pathlib import Path

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_test_cases(test_file: str = 'test_data.json') -> list:
    """Charge les cas de test depuis un fichier JSON"""
    test_path = Path(test_file)
    
    if not test_path.exists():
        logger.error(f"Fichier de test {test_file} introuvable")
        # Retourner des cas de test par d√©faut
        return [
            {
                "query_name": "Niacinamide 10% + Zinc 1%",
                "ingredients": "Aqua, Niacinamide, Pentylene Glycol, Zinc PCA, Tamarindus Indica Seed Gum, Xanthan Gum, Isoceteth-20, Ethoxydiglycol, Phenoxyethanol, Chlorphenesin",
                "expected_dupe": "The Ordinary",
                "category": "serum"
            }
        ]
    
    try:
        with open(test_path, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
        
        logger.info(f"‚úÖ {len(test_cases)} cas de test charg√©s depuis {test_file}")
        return test_cases
    except json.JSONDecodeError as e:
        logger.error(f"Erreur de parsing JSON: {e}")
        return []
    except Exception as e:
        logger.error(f"Erreur de chargement: {e}")
        return []

def calculate_search_metrics(engine: PureSkinNLPEngine, test_cases: list) -> dict:
    """Calcule les m√©triques de performance de recherche"""
    
    metrics = {
        "latencies": [],
        "ranks": [],
        "similarities": [],
        "top_1_hits": 0,
        "top_3_hits": 0,
        "top_5_hits": 0,
        "top_10_hits": 0,
        "not_found": 0,
        "failed_searches": 0
    }
    
    if not test_cases:
        logger.warning("Aucun cas de test disponible")
        return metrics
    
    logger.info(f"\nüöÄ D√âBUT DU BENCHMARK SUR {len(test_cases)} PRODUITS")
    logger.info("-" * 60)
    
    for idx, case in enumerate(test_cases, 1):
        query_name = case.get('query_name', f'Test {idx}')
        ingredients = case.get('ingredients', '')
        expected = case.get('expected_dupe', '')
        category = case.get('category', None)
        
        logger.debug(f"Traitement cas {idx}: {query_name}")
        
        if not ingredients:
            logger.warning(f"Cas {idx} ignor√©: pas d'ingr√©dients")
            continue
        
        # Mesure de la latence
        start_time = time.perf_counter()
        
        try:
            # Recherche avec cat√©gorie si sp√©cifi√©e
            results = engine.find_similar_products(
                target_ingredients=ingredients,
                secondary=category,
                top_n=10
            )
            
            duration = (time.perf_counter() - start_time) * 1000  # ms
            metrics["latencies"].append(duration)
            
        except Exception as e:
            logger.error(f"Erreur recherche cas {idx}: {e}")
            metrics["latencies"].append(float('inf'))
            metrics["failed_searches"] += 1
            results = []
            continue
        
        # Recherche du produit attendu
        expected_clean = expected.lower()
        found_rank = 0
        found_similarity = 0.0
        
        for i, res in enumerate(results, 1):
            full_name = f"{res.get('brand_name', '')} {res.get('product_name', '')}".lower()
            
            # Recherche par nom de marque ou produit
            if expected_clean in full_name or expected_clean in res.get('brand_name', '').lower():
                found_rank = i
                found_similarity = res.get('similarity', 0.0)
                break
        
        # Enregistrement des r√©sultats
        if found_rank > 0:
            metrics["ranks"].append(1.0 / found_rank)
            metrics["similarities"].append(found_similarity)
            
            # Mise √† jour des compteurs
            if found_rank == 1: 
                metrics["top_1_hits"] += 1
            if found_rank <= 3: 
                metrics["top_3_hits"] += 1
            if found_rank <= 5: 
                metrics["top_5_hits"] += 1
            if found_rank <= 10: 
                metrics["top_10_hits"] += 1
            
            icon = "ü•á" if found_rank == 1 else "ü•à" if found_rank <= 3 else "ü•â" if found_rank <= 5 else "‚úÖ"
            logger.info(f"{icon} '{query_name}' -> Trouv√© au rang #{found_rank} (similarit√©: {found_similarity:.3f})")
        else:
            metrics["ranks"].append(0.0)
            metrics["not_found"] += 1
            logger.info(f"‚ùå '{query_name}' -> Non trouv√© dans le Top 10")
    
    return metrics

def generate_report(metrics: dict, test_cases: list) -> None:
    """G√©n√®re un rapport d√©taill√© du benchmark"""
    
    total_cases = len(test_cases)
    if total_cases == 0:
        logger.error("Aucun cas trait√©")
        return
    
    # Calcul des m√©triques
    mrr_score = np.mean(metrics["ranks"]) if metrics["ranks"] else 0
    
    acc_top1 = (metrics["top_1_hits"] / total_cases) * 100
    acc_top3 = (metrics["top_3_hits"] / total_cases) * 100
    acc_top5 = (metrics["top_5_hits"] / total_cases) * 100
    acc_top10 = (metrics["top_10_hits"] / total_cases) * 100
    
    # Latence (filtrer les valeurs infinies)
    valid_latencies = [lat for lat in metrics["latencies"] if lat != float('inf')]
    avg_latency = np.mean(valid_latencies) if valid_latencies else 0
    latency_std = np.std(valid_latencies) if valid_latencies else 0
    latency_p95 = np.percentile(valid_latencies, 95) if valid_latencies else 0
    
    # Similarit√© moyenne
    avg_similarity = np.mean(metrics["similarities"]) if metrics["similarities"] else 0
    
    # Rapport
    logger.info("\n" + "="*50)
    logger.info("üìä R√âSULTATS DE PERFORMANCE PURESKIN")
    logger.info("="*50)
    
    logger.info(f"üìà STATISTIQUES G√âN√âRALES")
    logger.info(f"   Cas de test : {total_cases}")
    logger.info(f"   Recherches √©chou√©es : {metrics['failed_searches']}")
    logger.info(f"   Produits non trouv√©s : {metrics['not_found']} ({metrics['not_found']/total_cases*100:.1f}%)")
    
    logger.info(f"\n‚ö° PERFORMANCE TEMPORELLE")
    logger.info(f"   ‚è±Ô∏è  Latence moyenne : {avg_latency:.2f} ms")
    logger.info(f"   üìä √âcart-type : {latency_std:.2f} ms")
    logger.info(f"   üìà P95 (95% des requ√™tes) : {latency_p95:.2f} ms")
    
    logger.info(f"\nüèÜ QUALIT√â DE LA RECHERCHE")
    logger.info(f"   üéØ MRR Score : {mrr_score:.3f} / 1.000")
    logger.info(f"     - > 0.5 : Excellent")
    logger.info(f"     - 0.3-0.5 : Bon")
    logger.info(f"     - 0.1-0.3 : Moyen")
    logger.info(f"     - < 0.1 : Faible")
    
    logger.info(f"\nüéØ PR√âCISION PAR RANG")
    logger.info(f"   ü•á Top-1 Accuracy : {acc_top1:.1f}%")
    logger.info(f"   ü•à Top-3 Accuracy : {acc_top3:.1f}%")
    logger.info(f"   ü•â Top-5 Accuracy : {acc_top5:.1f}%")
    logger.info(f"   ‚úÖ Top-10 Accuracy : {acc_top10:.1f}%")
    
    logger.info(f"\nü§ù QUALIT√â DES CORRESPONDANCES")
    logger.info(f"   Similarit√© moyenne : {avg_similarity:.3f}")
    logger.info(f"   - > 0.8 : Correspondance excellente")
    logger.info(f"   - 0.6-0.8 : Bonne correspondance")
    logger.info(f"   - 0.4-0.6 : Correspondance mod√©r√©e")
    logger.info(f"   - < 0.4 : Correspondance faible")
    
    # Score global
    overall_score = (mrr_score + acc_top1/100 + avg_similarity) / 3
    logger.info(f"\n‚≠ê SCORE GLOBAL : {overall_score:.3f}/1.000")
    
    logger.info("="*50)
    
    # Sauvegarde des r√©sultats
    save_results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "test_cases": total_cases,
        "performance": {
            "mrr": mrr_score,
            "accuracy": {
                "top1": acc_top1,
                "top3": acc_top3,
                "top5": acc_top5,
                "top10": acc_top10
            },
            "latency_ms": {
                "mean": avg_latency,
                "std": latency_std,
                "p95": latency_p95
            },
            "similarity_mean": avg_similarity
        },
        "coverage": {
            "not_found": metrics["not_found"],
            "failed_searches": metrics["failed_searches"],
            "success_rate": 100 - ((metrics["not_found"] + metrics["failed_searches"]) / total_cases * 100)
        },
        "overall_score": overall_score
    }
    
    try:
        results_file = "benchmark_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_results, f, indent=2, ensure_ascii=False)
        logger.info(f"üíæ R√©sultats sauvegard√©s dans {results_file}")
    except Exception as e:
        logger.error(f"Erreur sauvegarde r√©sultats: {e}")

def main():
    """Fonction principale du benchmark"""
    
    # 1. Chargement du moteur
    logger.info("‚è≥ Chargement du moteur...")
    try:
        engine = PureSkinNLPEngine()
        engine_path = "pure_skin_engine.pt"
        
        if Path(engine_path).exists():
            engine.load_engine(engine_path)
            logger.info(f"‚úÖ Moteur charg√©: {len(engine.products_df_indexed)} produits")
        else:
            logger.error(f"‚ùå Fichier {engine_path} introuvable")
            logger.info("Tentative de chargement direct depuis CSV...")
            csv_path = Path("product_info_cleaned.csv")
            if csv_path.exists():
                import pandas as pd
                df = pd.read_csv(csv_path)
                engine.load_and_vectorize_data(df)
                logger.info(f"‚úÖ Donn√©es charg√©es depuis CSV: {len(df)} produits")
            else:
                logger.error("‚ùå Aucune donn√©e disponible")
                return
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement moteur: {e}")
        return
    
    # 2. Chargement des cas de test
    test_cases = load_test_cases('test_data.json')
    
    if not test_cases:
        logger.warning("Utilisation de cas de test par d√©faut...")
        test_cases = [
            {
                "query_name": "Test Standard",
                "ingredients": "Aqua, Glycerin, Niacinamide, Hyaluronic Acid",
                "expected_dupe": "The Ordinary",
                "category": "serum"
            }
        ]
    
    # 3. Ex√©cution du benchmark
    logger.info(f"üîç Ex√©cution du benchmark sur {len(test_cases)} cas...")
    metrics = calculate_search_metrics(engine, test_cases)
    
    # 4. G√©n√©ration du rapport
    generate_report(metrics, test_cases)
    
    # 5. Message final
    logger.info("\n‚ú® Benchmark termin√© avec succ√®s!")
    logger.info("üí° Conseils pour am√©liorer les r√©sultats:")
    logger.info("   - Augmenter la qualit√© des donn√©es d'entra√Ænement")
    logger.info("   - Ajouter plus de cas de test vari√©s")
    logger.info("   - Ajuster les param√®tres de pond√©ration dans clean_and_weight_ingredients()")
    logger.info("   - Consid√©rer l'ajout de features suppl√©mentaires (prix, marque, etc.)")

if __name__ == "__main__":
    main()